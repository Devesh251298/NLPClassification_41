{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/xy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/xy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/xy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/xy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder = \"dontpatronizeme_v1.4/\"\n",
    "output_data_folder = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # remove single quotes\n",
    "    text = re.sub(r'\\'', '', text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # remove new lines\n",
    "    text = re.sub(r'   ', ' ', text)\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    # remove stop words\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(path, col_names):\n",
    "\n",
    "    original_data=[]\n",
    "\n",
    "    with open (path) as data:\n",
    "        for line in data:\n",
    "            original_data.append(line)\n",
    "            \n",
    "    print('The original data contains ', len(original_data), ' lines.')\n",
    "    \n",
    "    lines = []\n",
    "\n",
    "    for line in original_data:\n",
    "        elements=line.strip().split('\\t')\n",
    "        lines.append(elements)\n",
    "    \n",
    "    df = pd.DataFrame(lines, columns = col_names)\n",
    "\n",
    "    # remove the 0-3 rows since they don't contain any data\n",
    "    df = df.iloc[4:].reset_index(drop=True)\n",
    "\n",
    "    # replace emptry cells with na\n",
    "    df = df.replace(r'^\\s*$', \"na\", regex=True)\n",
    "\n",
    "    # # remove rows where the \"text\" column is na\n",
    "    # df = df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_ids(path):\n",
    "    ids = []\n",
    "    num_lines = 0\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            if num_lines == 0:\n",
    "                num_lines += 1\n",
    "                continue\n",
    "            string = line.strip().split('\\t')[0]\n",
    "            ## get the number at the start of the string\n",
    "            string = int(string.split(',')[0])\n",
    "\n",
    "            ids.append(string)\n",
    "    return ids\n",
    "\n",
    "def preprocess_data(data_folder):\n",
    "    pcl_cols = [\"par_id\", \"art_id\", \"keyword\", \"country_code\", \"text\", \"label\"]\n",
    "    pcl_df = load_and_preprocess_data(f\"{data_folder}/dontpatronizeme_pcl.tsv\", pcl_cols)\n",
    "\n",
    "    ## preprocess the text\n",
    "    pcl_df['label'] = pcl_df['label'].astype(int)\n",
    "    pcl_df[\"class\"] = pcl_df.apply(lambda x: 1 if x[\"label\"] > 1 else 0, axis=1)\n",
    "\n",
    "    pcl_df[\"preprocessed_text\"] = pcl_df['text'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "    ## read txt files for train and dev paragraph ids in the raw data folder \n",
    "    ## ignore the first line and get all the ids\n",
    "    train_ids = get_ids(f\"{data_folder}/train_semeval_parids-labels.txt\")\n",
    "    dev_ids = get_ids(f\"{data_folder}/dev_semeval_parids-labels.txt\")\n",
    "\n",
    "    ## divide into train and dev according to provided ids \n",
    "    pcl_df['par_id'] = pcl_df['par_id'].astype(int)\n",
    "\n",
    "    train_indexes = pcl_df[pcl_df['par_id'].isin(train_ids)].index\n",
    "    dev_indexes = pcl_df[pcl_df['par_id'].isin(dev_ids)].index\n",
    "\n",
    "    train_df = pcl_df.iloc[train_indexes].reset_index(drop=True)\n",
    "    dev_df = pcl_df.iloc[dev_indexes].reset_index(drop=True)\n",
    "\n",
    "    ## divide train into train_train and train_dev\n",
    "    train_dev_df = train_df.sample(frac=0.2, random_state=42)\n",
    "    train_train_df = train_df.drop(train_dev_df.index).reset_index(drop=True)\n",
    "\n",
    "    print(pcl_df.dtypes)\n",
    "    print(pcl_df.shape)\n",
    "    print(pcl_df.isna().sum())\n",
    "\n",
    "    return train_train_df, train_dev_df, dev_df, pcl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data contains  10473  lines.\n",
      "par_id                int64\n",
      "art_id               object\n",
      "keyword              object\n",
      "country_code         object\n",
      "text                 object\n",
      "label                 int64\n",
      "class                 int64\n",
      "preprocessed_text    object\n",
      "dtype: object\n",
      "(10469, 8)\n",
      "par_id               0\n",
      "art_id               0\n",
      "keyword              0\n",
      "country_code         0\n",
      "text                 0\n",
      "label                0\n",
      "class                0\n",
      "preprocessed_text    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_train_df, train_dev_df, dev_df, pcl_df = preprocess_data(raw_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_train_df, train_dev_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8375, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of words in text column in dev_df\n",
    "dev_df[\"len_text\"] = dev_df['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess test data so we can make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, col_names):\n",
    "\n",
    "    original_data=[]\n",
    "\n",
    "    with open (path) as data:\n",
    "        for line in data:\n",
    "            original_data.append(line)\n",
    "            \n",
    "    print('The original data contains ', len(original_data), ' lines.')\n",
    "    \n",
    "    lines = []\n",
    "\n",
    "    for line in original_data:\n",
    "        elements=line.strip().split('\\t')\n",
    "        lines.append(elements)\n",
    "    \n",
    "    df = pd.DataFrame(lines, columns = col_names)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcl_test_cols = [\"par_id\", \"art_id\", \"keyword\", \"country_code\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data contains  3832  lines.\n"
     ]
    }
   ],
   "source": [
    "test_df = load_data(f\"{raw_data_folder}/task4_test.tsv\", pcl_test_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train_df.to_csv(f'{output_data_folder}/pcl_df_train_train_preprocessed.csv', index=False)\n",
    "train_dev_df.to_csv(f'{output_data_folder}/pcl_df_train_dev_preprocessed.csv', index=False)\n",
    "dev_df.to_csv(f'{output_data_folder}/pcl_df_dev_preprocessed.csv', index=False)\n",
    "pcl_df.to_csv(f'{output_data_folder}/pcl_df_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(f'{output_data_folder}/pcl_df_train_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(f'{output_data_folder}/pcl_df_test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dev set predictions - make sure par_id order is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ids = get_ids(f\"{raw_data_folder}/dev_semeval_parids-labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ids = pd.DataFrame(dev_ids, columns = [\"par_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dev.txt and assign \"pred_label\" as column name\n",
    "dev_preds = pd.read_csv(f'{output_data_folder}/dev.txt', names = [\"pred_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat dev_preds to dev_df as a new column\n",
    "dev_df_w_preds = pd.concat([dev_df, dev_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df_w_preds_ordered = dev_ids.merge(dev_df_w_preds, on='par_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country_code</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4046</td>\n",
       "      <td>@@14767805</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>us</td>\n",
       "      <td>We also know that they can benefit by receivin...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>also know benefit receiving counseling someone...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1279</td>\n",
       "      <td>@@7896098</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ng</td>\n",
       "      <td>Pope Francis washed and kissed the feet of Mus...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>pope francis washed kissed foot muslim orthodo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8330</td>\n",
       "      <td>@@17252299</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ng</td>\n",
       "      <td>Many refugees do n't want to be resettled anyw...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>many refugee nt want resettled anywhere let al...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4063</td>\n",
       "      <td>@@3002894</td>\n",
       "      <td>in-need</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>budding chef like fred winston angela kitchen ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4089</td>\n",
       "      <td>@@25597822</td>\n",
       "      <td>homeless</td>\n",
       "      <td>pk</td>\n",
       "      <td>\"In a 90-degree view of his constituency , one...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>degree view constituency one see high rise fly...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>10462</td>\n",
       "      <td>@@22092971</td>\n",
       "      <td>homeless</td>\n",
       "      <td>gh</td>\n",
       "      <td>The sad spectacle , which occurred on Saturday...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sad spectacle occurred saturday december repea...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>10463</td>\n",
       "      <td>@@4676355</td>\n",
       "      <td>refugee</td>\n",
       "      <td>pk</td>\n",
       "      <td>\"\"\" The Pakistani police came to our house and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pakistani police came house told u leave hoji ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>10464</td>\n",
       "      <td>@@19612634</td>\n",
       "      <td>disabled</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"When Marie O'Donoghue went looking for a spec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>marie odonoghue went looking special school in...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>10465</td>\n",
       "      <td>@@14297363</td>\n",
       "      <td>women</td>\n",
       "      <td>lk</td>\n",
       "      <td>\"Sri Lankan norms and culture inhibit women fr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sri lankan norm culture inhibit woman taking p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>10466</td>\n",
       "      <td>@@70091353</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ph</td>\n",
       "      <td>He added that the AFP will continue to bank on...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>added afp continue bank application whole nati...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2094 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      par_id      art_id     keyword country_code  \\\n",
       "0       4046  @@14767805    hopeless           us   \n",
       "1       1279   @@7896098     refugee           ng   \n",
       "2       8330  @@17252299     refugee           ng   \n",
       "3       4063   @@3002894     in-need           ie   \n",
       "4       4089  @@25597822    homeless           pk   \n",
       "...      ...         ...         ...          ...   \n",
       "2089   10462  @@22092971    homeless           gh   \n",
       "2090   10463   @@4676355     refugee           pk   \n",
       "2091   10464  @@19612634    disabled           ie   \n",
       "2092   10465  @@14297363       women           lk   \n",
       "2093   10466  @@70091353  vulnerable           ph   \n",
       "\n",
       "                                                   text  label  class  \\\n",
       "0     We also know that they can benefit by receivin...      3      1   \n",
       "1     Pope Francis washed and kissed the feet of Mus...      4      1   \n",
       "2     Many refugees do n't want to be resettled anyw...      2      1   \n",
       "3     \"Budding chefs , like \"\" Fred \"\" , \"\" Winston ...      4      1   \n",
       "4     \"In a 90-degree view of his constituency , one...      3      1   \n",
       "...                                                 ...    ...    ...   \n",
       "2089  The sad spectacle , which occurred on Saturday...      0      0   \n",
       "2090  \"\"\" The Pakistani police came to our house and...      0      0   \n",
       "2091  \"When Marie O'Donoghue went looking for a spec...      0      0   \n",
       "2092  \"Sri Lankan norms and culture inhibit women fr...      1      0   \n",
       "2093  He added that the AFP will continue to bank on...      0      0   \n",
       "\n",
       "                                      preprocessed_text  pred_label  \n",
       "0     also know benefit receiving counseling someone...         0.0  \n",
       "1     pope francis washed kissed foot muslim orthodo...         1.0  \n",
       "2     many refugee nt want resettled anywhere let al...         0.0  \n",
       "3     budding chef like fred winston angela kitchen ...         1.0  \n",
       "4     degree view constituency one see high rise fly...         0.0  \n",
       "...                                                 ...         ...  \n",
       "2089  sad spectacle occurred saturday december repea...         0.0  \n",
       "2090  pakistani police came house told u leave hoji ...         0.0  \n",
       "2091  marie odonoghue went looking special school in...         0.0  \n",
       "2092  sri lankan norm culture inhibit woman taking p...         0.0  \n",
       "2093  added afp continue bank application whole nati...         0.0  \n",
       "\n",
       "[2094 rows x 9 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df_w_preds_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas series to np array\n",
    "dev_predes_np = dev_df_w_preds_ordered[\"pred_label\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"dev.txt\", dev_predes_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
