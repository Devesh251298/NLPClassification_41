{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/xy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/xy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/xy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/xy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder = \"dontpatronizeme_v1.4/\"\n",
    "output_data_folder = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # remove single quotes\n",
    "    text = re.sub(r'\\'', '', text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # remove new lines\n",
    "    text = re.sub(r'   ', ' ', text)\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    # remove stop words\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(path, col_names):\n",
    "\n",
    "    original_data=[]\n",
    "\n",
    "    with open (path) as data:\n",
    "        for line in data:\n",
    "            original_data.append(line)\n",
    "            \n",
    "    print('The original data contains ', len(original_data), ' lines.')\n",
    "    \n",
    "    lines = []\n",
    "\n",
    "    for line in original_data:\n",
    "        elements=line.strip().split('\\t')\n",
    "        lines.append(elements)\n",
    "    \n",
    "    df = pd.DataFrame(lines, columns = col_names)\n",
    "\n",
    "    # remove the 0-3 rows since they don't contain any data\n",
    "    df = df.iloc[4:].reset_index(drop=True)\n",
    "\n",
    "    # replace emptry cells with na\n",
    "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "    # remove rows where the \"text\" column is na\n",
    "    df = df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_ids(path):\n",
    "    ids = []\n",
    "    num_lines = 0\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            if num_lines == 0:\n",
    "                num_lines += 1\n",
    "                continue\n",
    "            string = line.strip().split('\\t')[0]\n",
    "            ## get the number at the start of the string\n",
    "            string = int(string.split(',')[0])\n",
    "\n",
    "            ids.append(string)\n",
    "    return ids\n",
    "\n",
    "def preprocess_data(data_folder):\n",
    "    pcl_cols = [\"par_id\", \"art_id\", \"keyword\", \"country_code\", \"text\", \"label\"]\n",
    "    pcl_df = load_and_preprocess_data(f\"{data_folder}/dontpatronizeme_pcl.tsv\", pcl_cols)\n",
    "\n",
    "    ## preprocess the text\n",
    "    pcl_df['label'] = pcl_df['label'].astype(int)\n",
    "    pcl_df[\"class\"] = pcl_df.apply(lambda x: 1 if x[\"label\"] > 1 else 0, axis=1)\n",
    "\n",
    "    pcl_df[\"preprocessed_text\"] = pcl_df['text'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "    ## read txt files for train and dev paragraph ids in the raw data folder \n",
    "    ## ignore the first line and get all the ids\n",
    "    train_ids = get_ids(f\"{data_folder}/train_semeval_parids-labels.txt\")\n",
    "    dev_ids = get_ids(f\"{data_folder}/dev_semeval_parids-labels.txt\")\n",
    "\n",
    "    ## divide into train and dev according to provided ids \n",
    "    pcl_df['par_id'] = pcl_df['par_id'].astype(int)\n",
    "\n",
    "    train_indexes = pcl_df[pcl_df['par_id'].isin(train_ids)].index\n",
    "    dev_indexes = pcl_df[pcl_df['par_id'].isin(dev_ids)].index\n",
    "\n",
    "    train_df = pcl_df.iloc[train_indexes].reset_index(drop=True)\n",
    "    dev_df = pcl_df.iloc[dev_indexes].reset_index(drop=True)\n",
    "\n",
    "    ## divide train into train_train and train_dev\n",
    "    train_dev_df = train_df.sample(frac=0.2, random_state=42)\n",
    "    train_train_df = train_df.drop(train_dev_df.index).reset_index(drop=True)\n",
    "\n",
    "    print(pcl_df.dtypes)\n",
    "    print(pcl_df.shape)\n",
    "    print(pcl_df.isna().sum())\n",
    "\n",
    "    return train_train_df, train_dev_df, dev_df, pcl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data contains  10473  lines.\n",
      "par_id                int64\n",
      "art_id               object\n",
      "keyword              object\n",
      "country_code         object\n",
      "text                 object\n",
      "label                 int64\n",
      "class                 int64\n",
      "preprocessed_text    object\n",
      "dtype: object\n",
      "(10468, 8)\n",
      "par_id               0\n",
      "art_id               0\n",
      "keyword              0\n",
      "country_code         0\n",
      "text                 0\n",
      "label                0\n",
      "class                0\n",
      "preprocessed_text    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_train_df, train_dev_df, dev_df, pcl_df = preprocess_data(raw_data_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train_df.to_csv(f'{output_data_folder}/pcl_df_train_train_preprocessed.csv', index=False)\n",
    "train_dev_df.to_csv(f'{output_data_folder}/pcl_df_train_dev_preprocessed.csv', index=False)\n",
    "dev_df.to_csv(f'{output_data_folder}/pcl_df_dev_preprocessed.csv', index=False)\n",
    "pcl_df.to_csv(f'{output_data_folder}/pcl_df_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
