{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n",
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from googletrans import Translator # use version 4.0.0-rc1\n",
    "from dask import bag, diagnostics\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.flow as naf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_char_insertion(text):\n",
    "    aug = nac.KeyboardAug()\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text[0]\n",
    "\n",
    "def random_swap(text):\n",
    "    aug = naw.RandomWordAug(action=\"swap\")\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text[0]\n",
    "\n",
    "def synonym_replacement(text):\n",
    "    aug = naw.SynonymAug(aug_src='wordnet')\n",
    "    augmented_text = aug.augment(text)\n",
    "    return augmented_text[0]\n",
    "\n",
    "def back_translate(sequence, target_lang):\n",
    "\n",
    "    languages = ['en', 'fr', 'th', 'tr', 'ur', 'ru', 'bg', 'de', 'ar', 'zh-cn', 'hi',\n",
    "                 'sw', 'vi', 'es', 'el']\n",
    "    #instantiate translator\n",
    "    translator = Translator()\n",
    "    \n",
    "    #store original language so we can convert back\n",
    "    org_lang = translator.detect(sequence).lang\n",
    "\n",
    "    try:\n",
    "        if org_lang in languages:\n",
    "            #translate to new language and back to original\n",
    "            translated = translator.translate(sequence, dest = target_lang).text\n",
    "            #translate back to original language\n",
    "            translated_back = translator.translate(translated, dest = org_lang).text\n",
    "        \n",
    "            output_sequence = translated_back        \n",
    "        #if detected language not in our list of languages, do nothing\n",
    "        else:\n",
    "            output_sequence = sequence\n",
    "    except:\n",
    "        output_sequence = sequence\n",
    "    \n",
    "    return output_sequence\n",
    "\n",
    "# Applies above define function with Dask\n",
    "def back_translate_parallel(dataset, target_lang):\n",
    "    dataset = dataset.copy()\n",
    "    text_bag = bag.from_sequence(dataset['text'].tolist()).map(back_translate, target_lang)\n",
    "    \n",
    "    with diagnostics.ProgressBar():\n",
    "        text_bag = text_bag.compute()\n",
    "\n",
    "    # Add the translated to a new dataframe\n",
    "    df_augmented = pd.DataFrame({\"text\": text_bag, \"class\": dataset['class']})\n",
    "    return df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(pcl_df_train_train):\n",
    "\n",
    "    ## Back translation\n",
    "\n",
    "    for i in range(0,600,100):\n",
    "        pcl_df_train_train_aug = pcl_df_train_train[pcl_df_train_train['class'] == 1].iloc[i:i+100].copy()\n",
    "        pcl_df_train_train_aug.dropna(inplace=True)\n",
    "        pcl_df_train_train_aug = back_translate_parallel(pcl_df_train_train_aug, 'fr')\n",
    "\n",
    "        pcl_df_train_train_aug['class'] = 1\n",
    "\n",
    "        pcl_df_train_train = pd.concat([pcl_df_train_train, pcl_df_train_train_aug], ignore_index=True)\n",
    "\n",
    "    ## Synonym replacement \n",
    "\n",
    "    pcl_df_train_train_aug = pcl_df_train_train[pcl_df_train_train['class'] == 1].copy()\n",
    "    pcl_df_train_train_aug['text'] = pcl_df_train_train_aug['text'].apply(lambda x: synonym_replacement(x))\n",
    "    pcl_df_train_train_aug['class'] = 1\n",
    "\n",
    "    pcl_df_train_train = pd.concat([pcl_df_train_train, pcl_df_train_train_aug], ignore_index=True)\n",
    "\n",
    "    ## Random swap\n",
    "\n",
    "    pcl_df_train_train_aug = pcl_df_train_train[pcl_df_train_train['class'] == 1][:1000].copy()\n",
    "    pcl_df_train_train_aug['text'] = pcl_df_train_train_aug['text'].apply(lambda x: random_swap(x))\n",
    "    pcl_df_train_train_aug['class'] = 1\n",
    "\n",
    "    pcl_df_train_train = pd.concat([pcl_df_train_train, pcl_df_train_train_aug], ignore_index=True)\n",
    "\n",
    "    ## Random char insertion\n",
    "\n",
    "    pcl_df_train_train_aug = pcl_df_train_train[pcl_df_train_train['class'] == 1][:1000].copy()\n",
    "    pcl_df_train_train_aug['text'] = pcl_df_train_train_aug['text'].apply(lambda x: random_char_insertion(x))\n",
    "    pcl_df_train_train_aug['class'] = 1\n",
    "\n",
    "    pcl_df_train_train = pd.concat([pcl_df_train_train, pcl_df_train_train_aug], ignore_index=True)\n",
    "\n",
    "    return pcl_df_train_train    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT Text Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Text with ChatGPT, prompts are the definitions of the 3 types of PCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import openai\n",
    "\n",
    "# Add your OpenAI API key \n",
    "api_key = \"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\"\n",
    "\n",
    "class GenerateChatGPTtext():\n",
    "    def __init__(self, input,  num_samples, api_key):\n",
    "        self.input = input\n",
    "        self.num_samples = num_samples\n",
    "        openai.api_key = api_key\n",
    "    def generate_text(self, input):        \n",
    "        response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                            messages=[       \n",
    "                                            {\"role\": \"user\", \"content\": input}      \n",
    "                                            ])\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    def generate_samples(self):\n",
    "        results = []\n",
    "        while len(results) < self.num_samples:\n",
    "            try:\n",
    "                output = self.generate_text(self.input)\n",
    "                results.append(output)\n",
    "                print(output)\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                time.sleep(3)\n",
    "                continue \n",
    "        return results\n",
    "\n",
    "    def re_establish_connection(self):\n",
    "        openai.api_key = self.api_key\n",
    "        return openai.api_key\n",
    "\n",
    "\n",
    "    def remove_example(self, results):\n",
    "        for i in range(len(results)):\n",
    "            if results[i].startswith('Example: '):\n",
    "                results[i] = results[i][9:]\n",
    "            else:\n",
    "                continue\n",
    "        return results\n",
    "        \n",
    "saviour_input = \"The saviour is defined as The community which the author and the majority of the audience belong to is presented in some way as saviours of those vulnerable or in need. The language used subtly positionsthe author in a better, more privileged situation than the vulnerable community. They express thewill to help them, from their superior and advantageous position. Give an example of waht the saviour would say\"\n",
    "poet_input = \"The poet is a person who speaks in a patronising manner, The focus is not on the we (author and audience), but on the they (the individual or community referred to). The author uses a literary style to describe people or situations. They might, for example, use (or abuse) adjectives or rhetorical devices to either present a difficult situation as somehow beautiful, something to admire and learn from, or they might carefully detail its roughness to touch the heart of their audience. Give an example of what the poet would say\"\n",
    "expert_input = \"The expert is a person who speaks in a patronising manner, The underlying message is that the privileged community, which the author and their audience belong to, knows better what the vulnerable community needs, how they are or what they should do to overcome their situation. Give an example of what the expert would say\"\n",
    "\n",
    "\n",
    "inputs = [saviour_input, poet_input, expert_input]\n",
    "num_samples = 100\n",
    "full_results = []\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    gpt = GenerateChatGPTtext(inputs[i],  num_samples, api_key)\n",
    "    results = gpt.generate_samples()\n",
    "    results = gpt.remove_example(results)\n",
    "    full_results.extend(results)\n",
    "\n",
    "df = pd.DataFrame(full_results, columns=['text'])\n",
    "df['class'] = 1\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "df.to_csv('chatgpt_generated_text.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reword Every PCL Train Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  can you reword \"As chief minister of Rajasthan , Shekhawat had introduced ' Antodaya Yojna ' to fulfil the foodgrain needs of the poor , Gadkari said and described Shekhawat as a \"\" generous person \"\" .\"\n",
      "output:  \n",
      "\n",
      "Gadkari commended Shekhawat for implementing the 'Antodaya Yojna' program during his tenure as the chief minister of Rajasthan, which aimed to provide foodgrains for people in need. Additionally, Gadkari described Shekhawat as a kind-hearted individual.\n",
      "input:  can you reword The underprivileged also benefit a great deal at this time when donations and presents are distributed to the homeless and orphanages .\n",
      "output:  \n",
      "\n",
      "Those who are less fortunate also reap significant advantages during the time when contributions and gifts are given to homeless individuals and orphanages.\n",
      "input:  can you reword \"\"\" There are people who are struggling to make ends meet and it just seems ironic that in a country like Australia where we are so blessed with so much land and so much fertile country that we 're not able to get the food to people in need . \"\" <h> Feeding Sydney 's west\"\n",
      "output:  \n",
      "\n",
      "It seems paradoxical that in a land as bountiful as Australia, where there is plenty of fertile land, there are individuals who are finding it difficult to meet their basic needs. The perplexing reality is that we are unable to provide food to those who are in dire need, especially in regions like Sydney's western outskirts.\n",
      "input:  can you reword \"\"\" They are not humans to be looting something that is supposed to be delivered to their brothers in need , \"\" he went on to say .\"\n",
      "input:  can you reword \"\"\" They are not humans to be looting something that is supposed to be delivered to their brothers in need , \"\" he went on to say .\"\n",
      "output:  \n",
      "\n",
      "He continued to say that it is not humane for them to steal items intended for their needy brothers.\n",
      "input:  can you reword IT 'S a busy Tuesday evening , and like Jesus issuing out fish and loaves to the multitude , Donette Prendergast gently hands out boxes of cooked meals , cups of soup and drinks to a large group of homeless people at the post office on King Street in downtown Kingston .\n",
      "output:  \n",
      "\n",
      "As the bustling Tuesday night sets in, Donette Prendergast takes on the role of Jesus, distributing boxes of ready-to-eat meals, cups of warm soup, and refreshing drinks to the homeless community gathered at the King Street post office in downtown Kingston.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import openai\n",
    "\n",
    "# Add your OpenAI API key \n",
    "api_key = \"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\"\n",
    "\n",
    "class ReWordChatGPT():\n",
    "    def __init__(self,api_key, train_file):\n",
    "        self.input = input\n",
    "        openai.api_key = api_key\n",
    "        self.train_file = pd.read_csv(train_file)\n",
    "        self.train_list = list(self.train_file[self.train_file['class'] == 1]['text'])\n",
    "        \n",
    "    def generate_text(self, input):\n",
    "        response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
    "                                            messages=[       \n",
    "                                            {\"role\": \"user\", \"content\": input}      \n",
    "                                            ])\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "    def generate_random_reword_sample(self):\n",
    "        \"\"\" Reword every sample in the train data using the chatgpt model\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for i in range(len(self.train_list)):\n",
    "            sample_result = []\n",
    "            while len(sample_result) == 0: # keep trying until we get a result\n",
    "                try:\n",
    "                    input = f\"can you reword {self.train_list[i]}\"\n",
    "                    print('input: ', input)\n",
    "                    output = self.generate_text(input)\n",
    "                    sample_result.append(output)\n",
    "                    print('output: ', output)\n",
    "                    time.sleep(3)\n",
    "                except:\n",
    "                    time.sleep(3)\n",
    "                    continue \n",
    "            results.extend(sample_result)\n",
    "        return results\n",
    "\n",
    "\n",
    "train_file = 'archive\\pcl_df_train_train.csv'\n",
    "\n",
    "reword = ReWordChatGPT(api_key, train_file)\n",
    "results = reword.generate_random_reword_sample()\n",
    "# export to csv\n",
    "df = pd.DataFrame(results, columns=['text'])\n",
    "df['class'] = 1\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "df.to_csv('chatgpt_reworded_text.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
